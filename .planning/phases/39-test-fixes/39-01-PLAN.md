---
phase: 39-test-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/unit/indexer/test_symbols.py
  - tests/unit/test_cli.py
  - tests/unit/test_hybrid_search.py
autonomous: true

must_haves:
  truths:
    - "All unit tests pass without assertion failures"
    - "Symbol signature tests expect trailing colons matching implementation"
    - "CLI tests have complete Namespace mock objects"
    - "Hybrid search tests mock all required database paths"
  artifacts:
    - path: "tests/unit/indexer/test_symbols.py"
      provides: "Symbol extraction tests with correct format expectations"
      contains: "def foo():"
    - path: "tests/unit/test_cli.py"
      provides: "CLI command tests with complete Namespace mocks"
      contains: "before_context"
    - path: "tests/unit/test_hybrid_search.py"
      provides: "Hybrid search tests with complete mocks"
      contains: "apply_definition_boost"
  key_links:
    - from: "tests/unit/indexer/test_symbols.py"
      to: "src/cocosearch/indexer/symbols.py"
      via: "extract_symbol_metadata assertions"
      pattern: "symbol_signature.*==.*:"
    - from: "tests/unit/test_cli.py"
      to: "src/cocosearch/cli.py"
      via: "Namespace mock objects"
      pattern: "before_context.*None"
---

<objective>
Fix all test suite failures to achieve green test suite.

Purpose: Phase 40 (Code Cleanup) depends on passing tests. Cannot safely remove deprecated code without test verification.

Output: All 29 failing tests fixed, `poetry run pytest tests/unit` passes.
</objective>

<execution_context>
@/Users/fedorzhdanov/.claude/get-shit-done/workflows/execute-plan.md
@/Users/fedorzhdanov/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/39-test-fixes/39-CONTEXT.md
@.planning/phases/39-test-fixes/39-RESEARCH.md
@tests/unit/indexer/test_symbols.py
@tests/unit/test_cli.py
@tests/unit/test_hybrid_search.py
@src/cocosearch/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix symbol signature format tests</name>
  <files>tests/unit/indexer/test_symbols.py</files>
  <action>
Update symbol signature test expectations to match implementation format (implementation is source of truth per CONTEXT.md).

**14 failing tests need signature format updates:**

1. `TestFunctionExtraction.test_simple_function`:
   - Change: `"def foo()"` -> `"def foo():"`

2. `TestFunctionExtraction.test_function_with_parameters`:
   - Change: `"def bar(x, y=10)"` -> `"def bar(x, y=10):"`

3. `TestFunctionExtraction.test_function_with_type_hints`:
   - Change: `"def baz(x: int, y: str = 'default') -> dict"` -> `"def baz(x: int, y: str = 'default') -> dict:"`

4. `TestFunctionExtraction.test_async_function`:
   - Change: `"async def fetch(url: str) -> str"` -> `"async def fetch(url: str) -> str:"`

5. `TestFunctionExtraction.test_decorated_function`:
   - Change: `"def name(self)"` -> `"def name(self):"`

6. `TestFunctionExtraction.test_multiple_decorators`:
   - Already uses `in` assertion, should pass (verify)

7. `TestComplexCases.test_complex_type_hints`:
   - Uses `in` assertions, verify passes

8. `TestComplexCases.test_multiline_signature`:
   - Uses `in` assertions, verify passes

9. `TestJavaScriptSymbols.test_arrow_function_with_parens`:
   - Verify actual format and update expectation if needed

10. `TestTypeScriptSymbols.test_type_alias_simple`:
    - Change: `"type UserID"` -> actual format (check implementation output)

11. `TestGoSymbols.test_simple_function`:
    - Change: `"func Process()"` -> actual format (Go functions have different syntax)

12. `TestGoSymbols.test_method_with_pointer_receiver`:
    - Change: `"func Start()"` -> actual format

13. `TestRustSymbols.test_simple_function`:
    - Change: `"fn process()"` -> actual format

14. `TestRustSymbols.test_method_in_impl_block`:
    - Change: `"fn start(&self)"` -> actual format

**Approach:**
1. Run each failing test individually to see actual vs expected
2. Update expectation to match actual implementation output
3. Add trailing syntax characters as needed (`:` for Python, etc.)

**Why this approach:** Per CONTEXT.md, implementation is source of truth for format. Tests should match what the implementation produces.
  </action>
  <verify>
```bash
poetry run pytest tests/unit/indexer/test_symbols.py -v
```
All 93 symbol tests pass.
  </verify>
  <done>Zero symbol signature assertion failures. All TestFunctionExtraction, TestComplexCases, TestJavaScriptSymbols, TestTypeScriptSymbols, TestGoSymbols, TestRustSymbols tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Fix CLI Namespace mock tests</name>
  <files>tests/unit/test_cli.py</files>
  <action>
Add missing argparse.Namespace attributes to all mock objects to match actual parser definitions.

**12 failing tests need Namespace attribute additions:**

**TestSearchCommand tests (3 failures):**
1. `test_requires_query_without_interactive`
2. `test_json_output_is_valid`

Add these attributes to search command Namespace mocks:
```python
args = argparse.Namespace(
    query=...,
    index=...,
    limit=...,
    lang=None,
    min_score=0.3,
    context=None,           # -C flag
    before_context=None,    # -B flag (MISSING)
    after_context=None,     # -A flag (MISSING)
    no_smart=False,         # --no-smart (MISSING)
    pretty=False,
    interactive=False,
    hybrid=None,            # --hybrid (MISSING)
    symbol_type=None,       # --symbol-type (MISSING)
    symbol_name=None,       # --symbol-name (MISSING)
    no_cache=False,         # --no-cache (MISSING)
)
```

**TestStatsCommand tests (2 failures):**
1. `test_specific_index_json`
2. `test_nonexistent_index_error`

Add these attributes to stats command Namespace mocks:
```python
args = argparse.Namespace(
    index=...,
    pretty=False,
    verbose=False,          # -v (MISSING)
    json=False,             # --json (MISSING)
    all=False,              # --all (MISSING)
    staleness_threshold=7,  # --staleness-threshold (MISSING)
    live=False,             # --live (MISSING)
    watch=False,            # --watch (MISSING)
    refresh_interval=1.0,   # --refresh-interval (MISSING)
)
```

**TestErrorHandling tests (1 failure):**
1. `test_search_error_returns_json_error`
- Same attributes as TestSearchCommand

**TestMCPCommand tests (7 failures):**
1. `test_default_transport_is_stdio`
2. `test_transport_flag_overrides_env`
3. `test_env_transport_used_when_no_flag`
4. `test_port_flag_sets_port`
5. `test_port_env_used_when_no_flag`
6. `test_port_flag_overrides_env`
7. `test_default_port_is_3000`

Add this attribute to MCP command Namespace mocks:
```python
args = argparse.Namespace(
    transport=...,
    port=...,
    project_from_cwd=False,  # --project-from-cwd (MISSING)
)
```

**Why:** Mock Namespace objects must have ALL attributes that the command function accesses, even if None/False.
  </action>
  <verify>
```bash
poetry run pytest tests/unit/test_cli.py -v
```
All 52 CLI tests pass.
  </verify>
  <done>Zero AttributeError failures from missing Namespace attributes. TestSearchCommand, TestStatsCommand, TestErrorHandling, TestMCPCommand all pass.</done>
</task>

<task type="auto">
  <name>Task 3: Fix hybrid search test mocks</name>
  <files>tests/unit/test_hybrid_search.py</files>
  <action>
Fix 3 failing tests in TestHybridSearch class by adding missing mocks for `apply_definition_boost` code path.

**Root cause:** Tests mock `get_connection_pool` at `cocosearch.search.hybrid.get_connection_pool`, but `apply_definition_boost` calls `check_symbol_columns_exist` which imports `get_connection_pool` from `cocosearch.search.db`.

**3 failing tests:**
1. `test_hybrid_search_returns_semantic_only_when_no_keywords`
2. `test_hybrid_search_fuses_when_both_available`
3. `test_hybrid_search_respects_limit`

**Fix approach - add mock for db module's get_connection_pool:**
```python
with patch("cocosearch.search.hybrid.get_connection_pool", return_value=pool):
    with patch("cocosearch.search.db.get_connection_pool", return_value=pool):  # ADD THIS
        # ... rest of test
```

OR mock `apply_definition_boost` directly to return input unchanged:
```python
with patch("cocosearch.search.hybrid.apply_definition_boost", side_effect=lambda x, _: x):
    # ... rest of test
```

**Recommended approach:** Mock `apply_definition_boost` since it's simpler and avoids chasing internal implementation details. The function is not the focus of these tests.

**Alternative:** Mock at `cocosearch.search.db.get_connection_pool` if you want the apply_definition_boost logic to run.

**Why:** The hybrid_search function now calls apply_definition_boost which has its own database access path not covered by existing mocks.
  </action>
  <verify>
```bash
poetry run pytest tests/unit/test_hybrid_search.py -v
```
All 19 hybrid search tests pass.
  </verify>
  <done>Zero database connection errors in hybrid search tests. TestHybridSearch.test_hybrid_search_returns_semantic_only_when_no_keywords, test_hybrid_search_fuses_when_both_available, test_hybrid_search_respects_limit all pass.</done>
</task>

</tasks>

<verification>
Run full unit test suite to confirm all fixes:

```bash
poetry run pytest tests/unit -v --tb=short
```

Expected: 1022 tests pass, 0 failures.

Check for any regressions:
```bash
poetry run pytest tests/unit --tb=no -q
```

Should show: `1022 passed in X.XXs`
</verification>

<success_criteria>
1. `poetry run pytest tests/unit` exits with code 0
2. All 1022 unit tests pass
3. No new test failures introduced
4. Test changes match implementation behavior (impl is source of truth for format)
</success_criteria>

<output>
After completion, create `.planning/phases/39-test-fixes/39-01-SUMMARY.md`
</output>
