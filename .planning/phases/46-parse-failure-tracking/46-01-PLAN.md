---
phase: 46-parse-failure-tracking
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cocosearch/indexer/parse_tracking.py
  - src/cocosearch/indexer/schema_migration.py
  - src/cocosearch/indexer/flow.py
  - src/cocosearch/management/clear.py
autonomous: true

must_haves:
  truths:
    - "After indexing, each file has a parse_status value stored in the database"
    - "Parse status covers four categories: ok, partial, error, unsupported"
    - "Parse results table is dropped and recreated on each index run"
    - "Clearing an index also drops its parse_results table"
  artifacts:
    - path: "src/cocosearch/indexer/parse_tracking.py"
      provides: "detect_parse_status(), _collect_error_lines(), track_parse_results(), rebuild_parse_results()"
      min_lines: 80
    - path: "src/cocosearch/indexer/schema_migration.py"
      provides: "ensure_parse_results_table()"
      contains: "cocosearch_parse_results"
    - path: "src/cocosearch/indexer/flow.py"
      provides: "Parse tracking integration in run_index()"
      contains: "track_parse_results"
    - path: "src/cocosearch/management/clear.py"
      provides: "Parse results table cleanup on clear"
      contains: "cocosearch_parse_results"
  key_links:
    - from: "src/cocosearch/indexer/flow.py"
      to: "src/cocosearch/indexer/parse_tracking.py"
      via: "import and call track_parse_results() after flow.update()"
      pattern: "track_parse_results"
    - from: "src/cocosearch/indexer/parse_tracking.py"
      to: "src/cocosearch/indexer/symbols.py"
      via: "import LANGUAGE_MAP for extension-to-language mapping"
      pattern: "from cocosearch.indexer.symbols import LANGUAGE_MAP"
    - from: "src/cocosearch/indexer/flow.py"
      to: "src/cocosearch/indexer/schema_migration.py"
      via: "import and call ensure_parse_results_table() during setup"
      pattern: "ensure_parse_results_table"
---

<objective>
Create the parse failure tracking foundation: a standalone parse-status detection module, database schema migration for the parse_results table, and integration into the indexing and clearing pipelines.

Purpose: This is the data layer for parse failure tracking. Without it, no stats can be computed or displayed. Every downstream plan depends on this.
Output: New `parse_tracking.py` module, updated `schema_migration.py`, updated `flow.py` with post-index parse tracking, updated `clear.py` to drop parse_results on clear.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/46-parse-failure-tracking/46-CONTEXT.md
@.planning/phases/46-parse-failure-tracking/46-RESEARCH.md
@src/cocosearch/indexer/symbols.py
@src/cocosearch/indexer/schema_migration.py
@src/cocosearch/indexer/flow.py
@src/cocosearch/management/clear.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create parse_tracking.py module with detection and persistence functions</name>
  <files>src/cocosearch/indexer/parse_tracking.py</files>
  <action>
Create new module `src/cocosearch/indexer/parse_tracking.py` with the following functions:

1. `detect_parse_status(file_content: str, language_ext: str) -> tuple[str, str | None]`
   - Maps `language_ext` (e.g., "py", "ts") to tree-sitter language name using `LANGUAGE_MAP` from `symbols.py`
   - If extension not in `LANGUAGE_MAP`, return `("unsupported", None)`
   - Try parsing with `pack_get_parser(ts_language)` and `parser.parse(bytes(file_content, "utf8"))`
   - If `tree.root_node.has_error` is False, return `("ok", None)`
   - If `tree.root_node.has_error` is True, collect ERROR node line numbers with `_collect_error_lines()`, return `("partial", "ERROR nodes at lines: 5, 12, ...")`
   - On any exception, return `("error", str(e))`

2. `_collect_error_lines(node) -> list[int]`
   - Recursive walk of tree-sitter node tree
   - Collect line numbers (1-indexed via `node.start_point[0] + 1`) for nodes where `node.is_error or node.is_missing`
   - Return sorted list of line numbers

3. `track_parse_results(conn: psycopg.Connection, index_name: str, codebase_path: str, table_name: str) -> dict`
   - This is the main orchestration function called from `run_index()`
   - Query the chunks table (`table_name`) for `SELECT DISTINCT filename, language_id FROM {table_name}` to get all indexed files
   - For each file: join `codebase_path` with `filename` to get absolute path, read file content from disk, call `detect_parse_status()` with the `language_id` as the extension
   - Store the tree-sitter language name (from `LANGUAGE_MAP`, e.g., "python") in `parse_results.language`, NOT the extension. For unsupported languages, store the `language_id` value as-is (e.g., "dockerfile").
   - Call `rebuild_parse_results()` with collected results
   - Return summary dict: `{"total_files": N, "ok": N, "partial": N, "error": N, "unsupported": N}`

4. `rebuild_parse_results(conn: psycopg.Connection, index_name: str, results: list[dict]) -> None`
   - Table name: `cocosearch_parse_results_{index_name}`
   - TRUNCATE the table first (matches CONTEXT.md "rebuild on each index run" decision)
   - Batch insert all results using `executemany` with `INSERT INTO ... (file_path, language, parse_status, error_message) VALUES (%s, %s, %s, %s)`
   - Commit after insert

Import `LANGUAGE_MAP` from `cocosearch.indexer.symbols`. Import `pack_get_parser` from `tree_sitter_language_pack`. Use `logging.getLogger(__name__)` for logging. Log summary at INFO level after tracking completes.

IMPORTANT: Use `pathlib.Path(codebase_path) / filename` for path joining when reading files from disk. Wrap individual file reads in try/except -- if a file can't be read, record it as ("error", "FileNotFoundError: ...") rather than failing the entire tracking run.
  </action>
  <verify>
Run `python -c "from cocosearch.indexer.parse_tracking import detect_parse_status, track_parse_results, rebuild_parse_results; print('imports ok')"` to confirm the module loads without errors.

Run `python -c "
from cocosearch.indexer.parse_tracking import detect_parse_status
# Test ok parse
status, msg = detect_parse_status('def foo(): pass', 'py')
assert status == 'ok', f'Expected ok, got {status}'
assert msg is None

# Test unsupported
status, msg = detect_parse_status('FROM ubuntu', 'dockerfile')
assert status == 'unsupported', f'Expected unsupported, got {status}'

# Test partial (intentionally broken syntax)
status, msg = detect_parse_status('def foo(:\n    pass', 'py')
assert status == 'partial', f'Expected partial, got {status}'
assert 'ERROR' in msg

print('all assertions passed')
"` to verify detect_parse_status works correctly.
  </verify>
  <done>
  - `parse_tracking.py` exists with all four functions
  - `detect_parse_status()` correctly returns ok/partial/error/unsupported
  - Module imports cleanly
  </done>
</task>

<task type="auto">
  <name>Task 2: Add schema migration and integrate into flow.py and clear.py</name>
  <files>
    src/cocosearch/indexer/schema_migration.py
    src/cocosearch/indexer/flow.py
    src/cocosearch/management/clear.py
  </files>
  <action>
**schema_migration.py** -- Add `ensure_parse_results_table()` function:

```python
def ensure_parse_results_table(conn: psycopg.Connection, index_name: str) -> dict[str, Any]:
```
- Table name: `cocosearch_parse_results_{index_name}`
- CREATE TABLE IF NOT EXISTS with columns:
  - `file_path TEXT NOT NULL` (PRIMARY KEY)
  - `language TEXT NOT NULL`
  - `parse_status TEXT NOT NULL`
  - `error_message TEXT`
- CREATE INDEX IF NOT EXISTS `idx_cocosearch_parse_results_{index_name}_lang_status` ON the table `(language, parse_status)`
- Commit after creation
- Return `{"table_created": table_name}`
- Follow the exact same pattern as `ensure_hybrid_search_schema()` and `ensure_symbol_columns()` in the same file (idempotent, with logging).

**flow.py** -- Integrate parse tracking into `run_index()`:

1. Add import at top: `from cocosearch.indexer.schema_migration import ensure_parse_results_table` (add alongside existing import of `ensure_symbol_columns`)
2. Add import at top: `from cocosearch.indexer.parse_tracking import track_parse_results`
3. In the `with psycopg.connect(db_url) as conn:` block (after `ensure_symbol_columns(conn, table_name)`), add: `ensure_parse_results_table(conn, index_name)`
4. AFTER `update_info = flow.update()` (this is critical -- parse tracking runs AFTER indexing, not during), add a new block:

```python
# Track parse status for all indexed files
try:
    with psycopg.connect(db_url) as conn:
        parse_summary = track_parse_results(conn, index_name, codebase_path, table_name)
        logger.info(f"Parse tracking complete: {parse_summary}")
except Exception as e:
    logger.warning(f"Parse tracking failed (non-fatal): {e}")
```

The parse tracking is NON-FATAL. If it fails, indexing still succeeds. This matches the pattern used for cache invalidation in the same function.

**clear.py** -- Add parse_results table cleanup:

After the existing `DROP TABLE` statement for the chunks table, before the metadata cleanup block, add:

```python
# Drop parse results table if it exists (non-critical)
parse_table = f"cocosearch_parse_results_{index_name}"
try:
    cur.execute(f"DROP TABLE IF EXISTS {parse_table}")
    conn.commit()
except Exception:
    pass  # Table may not exist for pre-v46 indexes
```

This must happen INSIDE the existing `with pool.connection() as conn:` / `with conn.cursor() as cur:` block, right after the `cur.execute(f"DROP TABLE {table_name}")` and `conn.commit()` lines.
  </action>
  <verify>
Run `python -c "from cocosearch.indexer.schema_migration import ensure_parse_results_table; print('migration import ok')"` to verify the new function is importable.

Run `python -c "from cocosearch.indexer.flow import run_index; print('flow import ok')"` to verify flow.py still imports cleanly with the new imports.

Run `python -c "from cocosearch.management.clear import clear_index; print('clear import ok')"` to verify clear.py imports cleanly.

Run `grep -n 'track_parse_results\|ensure_parse_results_table' src/cocosearch/indexer/flow.py` to confirm both are referenced in the flow.
  </verify>
  <done>
  - `ensure_parse_results_table()` exists in schema_migration.py
  - `run_index()` calls `ensure_parse_results_table()` during setup and `track_parse_results()` after flow.update()
  - `clear_index()` drops the parse_results table alongside the chunks table
  - All three modules import cleanly
  </done>
</task>

</tasks>

<verification>
1. All new and modified Python files import without errors
2. `detect_parse_status()` correctly classifies ok, partial, error, and unsupported statuses
3. `flow.py` references both `ensure_parse_results_table` and `track_parse_results`
4. `clear.py` references `cocosearch_parse_results`
5. No existing imports or functionality are broken
</verification>

<success_criteria>
- parse_tracking.py module exists with detect_parse_status, track_parse_results, rebuild_parse_results
- schema_migration.py has ensure_parse_results_table function
- flow.py integrates parse tracking after flow.update() (non-fatal)
- clear.py drops parse_results table on index clear
- All modules import cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/46-parse-failure-tracking/46-01-SUMMARY.md`
</output>
