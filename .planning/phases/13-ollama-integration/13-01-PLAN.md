---
phase: 13-ollama-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - tests/fixtures/ollama_integration.py
  - tests/integration/conftest.py
autonomous: true

must_haves:
  truths:
    - "Native Ollama on localhost:11434 is detected before starting Docker container"
    - "Docker Ollama container starts when native unavailable"
    - "Session-scoped warmup runs once per test session (not per test)"
    - "First embedding request completes without timeout"
  artifacts:
    - path: "pyproject.toml"
      provides: "testcontainers[ollama] dependency"
      contains: "testcontainers[ollama]"
    - path: "tests/fixtures/ollama_integration.py"
      provides: "Ollama fixtures with native detection, Docker fallback, warmup"
      exports: ["is_ollama_available", "ollama_service", "warmed_ollama"]
    - path: "tests/integration/conftest.py"
      provides: "Registration of Ollama integration fixtures"
      contains: "tests.fixtures.ollama_integration"
  key_links:
    - from: "tests/fixtures/ollama_integration.py"
      to: "localhost:11434/api/tags"
      via: "httpx GET for native detection"
      pattern: "httpx.*api/tags"
    - from: "tests/fixtures/ollama_integration.py"
      to: "OllamaContainer"
      via: "testcontainers for Docker fallback"
      pattern: "OllamaContainer"
    - from: "tests/fixtures/ollama_integration.py"
      to: "cocoindex.functions.EmbedText"
      via: "warmup embedding request"
      pattern: "EmbedText.*OLLAMA"
---

<objective>
Create Ollama fixture infrastructure with native-first detection and Docker fallback

Purpose: Enable integration tests to generate real embeddings using either native Ollama or Docker container, with session-scoped warmup to prevent 30-second timeout on first request.

Output: Fixture module providing ollama_service (native or Docker URL) and warmed_ollama (pre-warmed Ollama ready for embedding requests)
</objective>

<execution_context>
@/Users/fedorzhdanov/.claude/get-shit-done/workflows/execute-plan.md
@/Users/fedorzhdanov/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-ollama-integration/13-CONTEXT.md
@.planning/phases/13-ollama-integration/13-RESEARCH.md
@tests/fixtures/containers.py
@tests/integration/conftest.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add testcontainers[ollama] dependency</name>
  <files>pyproject.toml</files>
  <action>
Add testcontainers[ollama]>=4.14.0 to the dev dependency group in pyproject.toml.

The package is already partially installed as testcontainers[postgres], so update the existing entry or add a separate one:
```toml
"testcontainers[postgres,ollama]>=4.14.0",
```

After updating pyproject.toml, run `uv sync --dev` to install the new dependency.

Verify the installation by running:
```python
from testcontainers.ollama import OllamaContainer
```
  </action>
  <verify>
Run: `uv run python -c "from testcontainers.ollama import OllamaContainer; print('OK')"`
Should print OK without import errors.
  </verify>
  <done>testcontainers[ollama] package installed and importable</done>
</task>

<task type="auto">
  <name>Task 2: Create Ollama integration fixtures</name>
  <files>tests/fixtures/ollama_integration.py, tests/integration/conftest.py</files>
  <action>
Create new file tests/fixtures/ollama_integration.py with these fixtures (following Phase 12 patterns):

**1. is_ollama_available() function:**
- Check if Ollama responds on localhost:11434
- Use httpx GET to /api/tags endpoint (lightweight health check)
- 2-second timeout (sufficient for local check)
- Return True if status 200, False on timeout/connection error

**2. ollama_service fixture (session-scoped):**
- First check is_ollama_available() for native Ollama
- If native available: yield "http://localhost:11434"
- If native unavailable: start OllamaContainer
  - Pull nomic-embed-text model if not present (274MB download)
  - Yield container URL (get_container_host_ip() + get_exposed_port(11434))
- Handle pull failures with pytest.skip() and clear message

**3. warmed_ollama fixture (session-scoped, depends on ollama_service):**
- Set OLLAMA_HOST environment variable to ollama_service URL
- Make throwaway embedding request using cocoindex.functions.EmbedText
  - api_type=cocoindex.LlmApiType.OLLAMA
  - model="nomic-embed-text"
  - Use httpx timeout of 60s for first request (model load)
- If warmup fails: pytest.skip() with error message
- Yield ollama_service URL for tests to use

**Locked decisions from CONTEXT.md:**
- Native-first detection via localhost:11434 health check
- Docker fallback when native unavailable
- Session scope for performance (container persists for session)
- Both pre-warm AND extended timeout approach
- 60s timeout for safety on first request

**Update tests/integration/conftest.py:**
Add "tests.fixtures.ollama_integration" to pytest_plugins list to register the new fixtures.

Note: Create a NEW file ollama_integration.py separate from existing ollama.py (which has unit test mocks). Do NOT modify the existing ollama.py file.
  </action>
  <verify>
1. Run: `uv run python -c "from tests.fixtures.ollama_integration import is_ollama_available, ollama_service, warmed_ollama; print('OK')"`
2. Check imports work: `uv run python -c "import httpx; from testcontainers.ollama import OllamaContainer; import cocoindex; print('OK')"`
3. Verify conftest.py has both container fixture registrations
  </verify>
  <done>
- is_ollama_available() function detects native Ollama via /api/tags
- ollama_service fixture provides URL (native or Docker)
- warmed_ollama fixture makes warmup request with 60s timeout
- Fixtures registered in integration conftest.py
  </done>
</task>

</tasks>

<verification>
1. Run: `uv run python -c "from tests.fixtures.ollama_integration import is_ollama_available; print('Native:', is_ollama_available())"`
   Should print True if Ollama running locally, False otherwise

2. Run: `uv run pytest tests/integration/conftest.py --collect-only`
   Should not error (fixtures can be collected)

3. Import check: `uv run python -c "from testcontainers.ollama import OllamaContainer; print('OK')"`
</verification>

<success_criteria>
- testcontainers[ollama]>=4.14.0 in pyproject.toml dev dependencies
- tests/fixtures/ollama_integration.py exists with is_ollama_available, ollama_service, warmed_ollama
- tests/integration/conftest.py registers ollama_integration fixtures
- Native detection uses httpx GET to localhost:11434/api/tags
- Docker fallback uses OllamaContainer with model pull
- Warmup uses 60s timeout via cocoindex EmbedText
</success_criteria>

<output>
After completion, create `.planning/phases/13-ollama-integration/13-01-SUMMARY.md`
</output>
